import asyncio
import time
import io
import logging
from datetime import datetime
from typing import Optional, List, Dict, Any
import numpy as np
from PIL import Image
import tensorflow as tf
from tensorflow.keras.models import load_model

from core.config import settings
from core.models import PredictionResult, PredictionResponse, PredictionStatus

logger = logging.getLogger(__name__)

class PredictionService:
    """Service de pr√©diction pour la classification d'images de jeux vid√©o"""
    
    def __init__(self):
        self.model = None
        self.categories = settings.MODEL_CATEGORIES
        self.image_size = settings.IMAGE_SIZE
        self.is_model_loaded = False
        self.prediction_history = []  # En production, utiliser une base de donn√©es
    
    async def load_model(self):
        """Chargement du mod√®le TensorFlow"""
        if settings.ENVIRONMENT == "test":
            import inspect

            for frame in inspect.stack():
                if "test_admin" in frame.filename:
                    logger.info("Test admin d√©tect√© : on charge le vrai mod√®le m√™me en mode test.")
                    break
                else:
                    logger.info("Mode test d√©tect√© : chargement du mod√®le ignor√©.")
            
                    # faux mod√®le ave m√©thode predict simul√©e
                    class DummyModel:
                        def predict(self, x, verbose=0):
                            dummy_probs = np.array([[0.1] * len(settings.MODEL_CATEGORIES)])
                            dummy_probs[0][0] = 0.9
                            return dummy_probs
                    
                    self.model = DummyModel()
                    self.is_model_loaded = True
                    return
            
            try:
                logger.info(f"üîÑ Chargement du mod√®le : {settings.MODEL_PATH}")
            
                # Chargement asynchrone du mod√®le
                loop = asyncio.get_event_loop()
                self.model = await loop.run_in_executor(
                    None, 
                    lambda: load_model(settings.MODEL_PATH)
                )
                
                self.is_model_loaded = True
                logger.info("‚úÖ Mod√®le charg√© avec succ√®s")
                
                # Test du mod√®le avec une image factice
                await self._test_model()
            
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du chargement du mod√®le : {str(e)}")
                raise Exception(f"Impossible de charger le mod√®le : {str(e)}")
        
        async def _test_model(self):
            """Test du mod√®le avec une image factice"""
            try:
                # Cr√©ation d'une image de test
                test_image = np.random.rand(*self.image_size, 3).astype(np.float32)
                test_image = np.expand_dims(test_image, axis=0)
                
                # Pr√©diction de test
                loop = asyncio.get_event_loop()
                prediction = await loop.run_in_executor(
                    None,
                    lambda: self.model.predict(test_image, verbose=0)
                )
                
                logger.info("‚úÖ Test du mod√®le r√©ussi")
                
            except Exception as e:
                logger.error(f"‚ùå √âchec du test du mod√®le : {str(e)}")
                raise Exception(f"Le mod√®le ne fonctionne pas correctement : {str(e)}")
    
    def _preprocess_image(self, image_bytes: bytes) -> np.ndarray:
        """Pr√©traitement de l'image pour la pr√©diction"""
        try:
            # Chargement de l'image depuis les bytes
            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
            
            # Redimensionnement
            image = image.resize(self.image_size)
            
            # Conversion en array numpy et normalisation
            image_array = np.array(image) / 255.0
            
            # Ajout de la dimension batch
            image_array = np.expand_dims(image_array, axis=0)
            
            return image_array
            
        except Exception as e:
            logger.error(f"Erreur pr√©traitement image : {str(e)}")
            raise Exception(f"Impossible de traiter l'image : {str(e)}")
    
    async def predict_image(
        self, 
        image_bytes: bytes, 
        user_id: int,
        filename: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Pr√©diction de cat√©gorie pour une image
        
        Args:
            image_bytes: Donn√©es binaires de l'image
            user_id: ID de l'utilisateur
            filename: Nom du fichier (optionnel)
            
        Returns:
            Dictionnaire avec les r√©sultats de pr√©diction
        """
        start_time = time.time()
        
        try:
            if not self.is_model_loaded:
                raise Exception("Mod√®le non charg√©")
            
            # Validation de la taille du fichier
            if len(image_bytes) > settings.MAX_FILE_SIZE:
                raise Exception(f"Fichier trop volumineux (max: {settings.MAX_FILE_SIZE / (1024*1024):.1f}MB)")
            
            # Pr√©traitement de l'image
            processed_image = self._preprocess_image(image_bytes)
            
            # Pr√©diction asynchrone
            loop = asyncio.get_event_loop()
            predictions = await loop.run_in_executor(
                None,
                lambda: self.model.predict(processed_image, verbose=0)
            )
            
            # Traitement des r√©sultats
            probabilities = predictions[0]
            predicted_class_idx = np.argmax(probabilities)
            predicted_category = self.categories[predicted_class_idx]
            confidence = float(probabilities[predicted_class_idx])
            
            # Cr√©ation du dictionnaire des probabilit√©s
            prob_dict = {
                category: float(prob) 
                for category, prob in zip(self.categories, probabilities)
            }
            
            # Cr√©ation du r√©sultat
            prediction_result = PredictionResult(
                category=predicted_category,
                confidence=confidence,
                probabilities=prob_dict
            )
            
            # Calcul du temps de traitement
            processing_time = time.time() - start_time
            
            # Cr√©ation de la r√©ponse
            response = {
                "filename": filename,
                "prediction": prediction_result.dict(),
                "processing_time": processing_time,
                "timestamp": datetime.utcnow(),
                "user_id": user_id,
                "status": PredictionStatus.SUCCESS
            }
            
            # Sauvegarde dans l'historique
            await self._save_prediction_history(response)
            
            logger.info(
                f"Pr√©diction r√©ussie - Utilisateur: {user_id}, "
                f"Cat√©gorie: {predicted_category}, Confiance: {confidence:.3f}"
            )
            
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_response = {
                "filename": filename,
                "prediction": None,
                "processing_time": processing_time,
                "timestamp": datetime.utcnow(),
                "user_id": user_id,
                "status": PredictionStatus.ERROR,
                "error_message": str(e)
            }
            
            logger.error(f"Erreur pr√©diction - Utilisateur: {user_id}, Erreur: {str(e)}")
            
            # Sauvegarde de l'erreur dans l'historique
            await self._save_prediction_history(error_response)
            
            raise Exception(str(e))
    
    async def _save_prediction_history(self, prediction_data: Dict[str, Any]):
        """Sauvegarde de l'historique des pr√©dictions"""
        try:
            # En production, sauvegarder en base de donn√©es
            self.prediction_history.append(prediction_data)
            
            # Limitation de l'historique en m√©moire (garder les 1000 derni√®res)
            if len(self.prediction_history) > 1000:
                self.prediction_history = self.prediction_history[-1000:]
                
        except Exception as e:
            logger.error(f"Erreur sauvegarde historique : {str(e)}")
    
    async def get_user_history(self, user_id: int, limit: int = 50) -> List[Dict[str, Any]]:
        """R√©cup√©ration de l'historique des pr√©dictions d'un utilisateur"""
        try:
            # Filtrage par utilisateur
            user_predictions = [
                pred for pred in self.prediction_history 
                if pred.get("user_id") == user_id
            ]
            
            # Tri par timestamp d√©croissant et limitation
            user_predictions.sort(key=lambda x: x.get("timestamp", datetime.min), reverse=True)
            
            return user_predictions[:limit]
            
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration historique : {str(e)}")
            return []
    
    async def get_prediction_count(self) -> int:
        """Nombre total de pr√©dictions"""
        return len([pred for pred in self.prediction_history if pred.get("status") == PredictionStatus.SUCCESS])
    
    async def get_predictions_today(self) -> int:
        """Nombre de pr√©dictions aujourd'hui"""
        today = datetime.utcnow().date()
        return len([
            pred for pred in self.prediction_history 
            if (pred.get("timestamp", datetime.min).date() == today and 
                pred.get("status") == PredictionStatus.SUCCESS)
        ])
    
    async def get_top_categories(self, limit: int = 5) -> Dict[str, int]:
        """Top des cat√©gories les plus pr√©dites"""
        try:
            category_counts = {}
            
            for pred in self.prediction_history:
                if pred.get("status") == PredictionStatus.SUCCESS and pred.get("prediction"):
                    category = pred["prediction"].get("category")
                    if category:
                        category_counts[category] = category_counts.get(category, 0) + 1
            
            # Tri par nombre d√©croissant
            sorted_categories = sorted(
                category_counts.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            return dict(sorted_categories[:limit])
            
        except Exception as e:
            logger.error(f"Erreur calcul top cat√©gories : {str(e)}")
            return {}
    
    async def get_model_info(self) -> Dict[str, Any]:
        """Informations sur le mod√®le charg√©"""
        if not self.is_model_loaded:
            return {"status": "not_loaded"}
        
        try:
            return {
                "status": "loaded",
                "categories": self.categories,
                "image_size": self.image_size,
                "model_path": settings.MODEL_PATH,
                "total_predictions": await self.get_prediction_count(),
                "predictions_today": await self.get_predictions_today()
            }
        except Exception as e:
            logger.error(f"Erreur info mod√®le : {str(e)}")
            return {"status": "error", "error": str(e)}
    
    def health_check(self) -> Dict[str, Any]:
        """V√©rification de l'√©tat de sant√© du service"""
        return {
            "service": "prediction_service",
            "status": "healthy" if self.is_model_loaded else "unhealthy",
            "model_loaded": self.is_model_loaded,
            "categories_count": len(self.categories),
            "history_count": len(self.prediction_history)
        } 