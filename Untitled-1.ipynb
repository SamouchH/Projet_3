{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d13684a",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark initialisation (session + context)\n",
    "from pyspark.sql import SparkSession\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"GPS_App_Analysis\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Download the dataset\n",
    "def download_file(filename: str) -> None:\n",
    "    base_url = \"https://assets-datascientest.s3.eu-west-1.amazonaws.com/\"\n",
    "    urlretrieve(base_url + filename, filename)\n",
    "\n",
    "download_file(\"gps_app.csv\")\n",
    "\n",
    "# Load the CSV into a Spark DataFrame\n",
    "raw_app = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(\"gps_app.csv\")\n",
    ")\n",
    "\n",
    "raw_app.show(5)\n",
    "raw_app.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dedea6",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86222ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer toutes les colonnes : espaces → underscores, majuscules → minuscules\n",
    "raw_app = raw_app.toDF(*[col.replace(\" \", \"_\").lower() for col in raw_app.columns])\n",
    "\n",
    "# Vérification\n",
    "raw_app.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b721e5",
   "metadata": {},
   "source": [
    "Q3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, isnan, count, expr, percentile_approx\n",
    "\n",
    "# Calcul de la médiane de rating\n",
    "median_rating = raw_app.select(percentile_approx(\"rating\", 0.5)).first()[0]\n",
    "\n",
    "# Remplacement des NaN par la médiane\n",
    "raw_app = raw_app.fillna({\"rating\": median_rating})\n",
    "\n",
    "# Vérification\n",
    "raw_app.select(\"rating\").summary(\"count\", \"mean\", \"min\", \"max\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, mean, min, max, percentile_approx\n",
    "\n",
    "# 1. Compter les valeurs nulles / non nulles dans rating\n",
    "print(\" Statistiques de présence :\")\n",
    "raw_app.select(\n",
    "    count(\"rating\").alias(\"non_nulls\"),\n",
    "    count(\"*\").alias(\"total\"),\n",
    ").withColumn(\"missing\", col(\"total\") - col(\"non_nulls\")).show()\n",
    "\n",
    "# 2. Statistiques descriptives classiques\n",
    "print(\" Statistiques descriptives de rating :\")\n",
    "raw_app.select(\"rating\").summary(\"count\", \"mean\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()\n",
    "\n",
    "# 3. Table de fréquence des ratings\n",
    "print(\" Distribution des valeurs de rating (fréquences) :\")\n",
    "raw_app.groupBy(\"rating\").count().orderBy(\"rating\").show(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc443e9",
   "metadata": {},
   "source": [
    "Q3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    strategy=\"median\",\n",
    "    inputCols=[\"rating\"],\n",
    "    outputCols=[\"rating\"]\n",
    ")\n",
    "raw_app = imputer.fit(raw_app).transform(raw_app)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 1. Afficher les valeurs distinctes de la colonne \"type\"\n",
    "raw_app.select(\"type\").distinct().show()\n",
    "\n",
    "# 2. Compter les occurrences de chaque type pour identifier la modalité la plus fréquente\n",
    "raw_app.groupBy(\"type\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# 3. Vérifier quelles lignes sont manquantes\n",
    "raw_app.filter(col(\"type\").isNull() | (col(\"type\") == \"\")).show()\n",
    "\n",
    "# 4. Calculer la modalité la plus fréquente (le mode)\n",
    "mode_type = raw_app.groupBy(\"type\").count().orderBy(col(\"count\").desc()).first()[0]\n",
    "\n",
    "raw_app = raw_app.withColumn(\n",
    "    \"type\",\n",
    "    when(col(\"type\").isNull() | (col(\"type\") == \"\"), mode_type)\n",
    "    .otherwise(col(\"type\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc2179d",
   "metadata": {},
   "source": [
    "Q3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Afficher les valeurs uniques de \"type\"\n",
    "raw_app.select(\"type\").distinct().show()\n",
    "\n",
    "# 2. On remarque par exemple qu’il y a une valeur vide (\"\") ou un None en plus de \"Free\" et \"Paid\"\n",
    "#    Ce même enregistrement a aussi content_rating = null.\n",
    "\n",
    "# 3. Filtrer pour ne garder que les types valides (\"Free\" et \"Paid\")\n",
    "raw_app = raw_app.filter(col(\"type\").isin(\"Free\", \"Paid\"))\n",
    "\n",
    "# 4. Vérifier que le problème est réglé pour \"type\" ET pour \"content_rating\"\n",
    "raw_app.select(\"type\").distinct().show()\n",
    "raw_app.select(\"content_rating\").filter(col(\"content_rating\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# 1. Afficher les valeurs distinctes de content_rating\n",
    "raw_app.select(\"content_rating\").distinct().show()\n",
    "\n",
    "# 2. Afficher la répartition (effectifs) de chaque modalité\n",
    "raw_app.groupBy(\"content_rating\") \\\n",
    "       .count() \\\n",
    "       .orderBy(desc(\"count\")) \\\n",
    "       .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f890d",
   "metadata": {},
   "source": [
    "Q3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, desc\n",
    "\n",
    "# 1. Calculer la modalité (valeur la plus fréquente) de current_ver\n",
    "mode_current = (\n",
    "    raw_app.groupBy(\"current_ver\")\n",
    "           .count()\n",
    "           .orderBy(desc(\"count\"))\n",
    "           .first()[0]\n",
    ")\n",
    "\n",
    "# 2. Calculer la modalité de android_ver\n",
    "mode_android = (\n",
    "    raw_app.groupBy(\"android_ver\")\n",
    "           .count()\n",
    "           .orderBy(desc(\"count\"))\n",
    "           .first()[0]\n",
    ")\n",
    "\n",
    "# 3. Imputer les valeurs manquantes pour current_ver et android_ver\n",
    "raw_app = (\n",
    "    raw_app\n",
    "    .withColumn(\n",
    "        \"current_ver\",\n",
    "        when(col(\"current_ver\").isNull(), mode_current)\n",
    "        .otherwise(col(\"current_ver\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"android_ver\",\n",
    "        when(col(\"android_ver\").isNull(), mode_android)\n",
    "        .otherwise(col(\"android_ver\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7534c9a",
   "metadata": {},
   "source": [
    "Q3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28031e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, sum as _sum, when, isnan\n",
    "\n",
    "\n",
    "# Définitions des fonctions\n",
    "def getMissingValues(df):\n",
    "    exprs = [\n",
    "        _sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    missing_dict = df.select(*exprs).collect()[0].asDict()\n",
    "    rows = [(k, v) for k, v in missing_dict.items()]\n",
    "    return spark.createDataFrame(rows, [\"column\", \"missingCount\"])\n",
    "\n",
    "def missingTable(missing_df):\n",
    "    missing_df.filter(col(\"missingCount\") > 0).show(truncate=False)\n",
    "\n",
    "# Puis exécute :\n",
    "missingTable(getMissingValues(raw_app))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3574b8",
   "metadata": {},
   "source": [
    "Q4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7aa375",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449ef3f",
   "metadata": {},
   "source": [
    "Q4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66669df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "raw_cc = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"creditcard.csv\")\n",
    "\n",
    "# 3. Nettoyage (suppression des lignes avec valeurs manquantes)\n",
    "cc = raw_cc.dropna()\n",
    "\n",
    "# 4. Split train/test\n",
    "train, test = cc.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 5. Assemblage des features\n",
    "feature_cols = [c for c in cc.columns if c not in (\"Amount\", \"Class\")]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled\")\n",
    "scaler = StandardScaler(inputCol=\"assembled\", outputCol=\"features\")\n",
    "\n",
    "# 6. Modèle de régression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Amount\")\n",
    "\n",
    "# 7. Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# 8. Paramètres pour validation croisée\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.1, 0.01])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# 9. Evaluateur\n",
    "evaluator = RegressionEvaluator(labelCol=\"Amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# 10. CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "# 11. Entraînement\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# 12. Prédiction et évaluation\n",
    "predictions = cvModel.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = RegressionEvaluator(labelCol=\"Amount\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "\n",
    "print(f\"RMSE sur le test : {rmse:.4f}\")\n",
    "print(f\"R2 sur le test  : {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
